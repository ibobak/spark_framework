{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a draft version; this notebook will be soon extended with real working code that \n",
    "# uses data files and shows the examples of how to transform the data.\n",
    "# Be patient, please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of how the Jupyter kernel for this notebook can be configured path to the file with this content (in my case python was installed to /opt/anaconda3): \n",
    "`/opt/anaconda3/share/jupyter/kernels/pyspark-cluster-64cores-8ex/kernel.json`\n",
    "\n",
    "WARNINGS: \n",
    "- replace \"yourenv\" with your environment name, replace \"yourhostname\" with your real host name\n",
    "- replace /opt/spark-3.0.0-bin-without-hadoop-local  with your path to Spark and the version\n",
    "   py4j-0.10.9-src.zip to your actual version\n",
    "\n",
    "\n",
    "Contents of the kernel.json:\n",
    "\n",
    "`{\n",
    " \"display_name\": \"pyspark-cluster-64cores-8ex\",\n",
    " \"language\": \"python\",\n",
    " \"argv\": [\n",
    "    \"/opt/anaconda3/envs/yourenv/bin/python\",\n",
    "    \"-m\",\n",
    "    \"ipykernel\",\n",
    "    \"-f\",\n",
    "    \"{connection_file}\"\n",
    " ],\n",
    " \"env\": {\n",
    "    \"ENV_NAME\": \"dev\",\n",
    "    \"CAPTURE_STANDARD_OUT\": \"true\",\n",
    "    \"CAPTURE_STANDARD_ERR\": \"true\",\n",
    "    \"SEND_EMPTY_OUTPUT\": \"false\",\n",
    "    \"PYSPARK_PYTHON\": \"/opt/anaconda3/envs/yourenv/bin/python\",\n",
    "    \"PYSPARK_DRIVER_PYTHON\": \"/opt/anaconda3/envs/yourenv/bin/python\",\n",
    "    \"SPARK_HOME\": \"/opt/spark-3.0.0-bin-without-hadoop-local\",\n",
    "    \"PYTHONPATH\": \"/opt/spark-3.0.0-bin-without-hadoop-local/python/:/opt/spark-3.0.0-bin-without-hadoop-local/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"PYTHONSTARTUP\": \"/opt/spark-3.0.0-bin-without-hadoop-local/python/pyspark/shell.py\",\n",
    "    \"PYSPARK_SUBMIT_ARGS\": \"--driver-memory 8G --executor-memory 28G --num-executors 8  --executor-cores 8 --total-executor-cores 64 --conf spark.sql.shuffle.partitions=300 --conf spark.default.parallelism=300 --conf \\\"spark.executor.extraJavaOptions=-Djava.io.tmpdir=/mnt/sparkExecutorTmp\\\" --conf \\\"spark.driver.extraJavaOptions=-Djava.io.tmpdir=/mnt/sparkDriverTmp\\\" --packages io.delta:delta-core_2.12:0.7.0 --master spark://yourhostname:7077 pyspark-shell\"\n",
    " }\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-16T21:10:14.302Z"
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-16T21:10:04.758Z"
    }
   },
   "outputs": [],
   "source": [
    "# before using functions from the spark_framework package, you need to call init(spark) \n",
    "# to pass the reference to the spark session \n",
    "from spark_framework import *\n",
    "init(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T12:53:14.025138Z",
     "start_time": "2019-07-10T12:53:14.021011Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if \"PYSPARK_SUBMIT_ARGS\" in os.environ:\n",
    "    print(os.environ[\"PYSPARK_SUBMIT_ARGS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T12:53:14.913195Z",
     "start_time": "2019-07-10T12:53:14.523346Z"
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:03:11.086512Z",
     "start_time": "2019-07-10T13:03:11.083364Z"
    }
   },
   "outputs": [],
   "source": [
    "# for local filesystem connections we need to set type=\"local\" and \"url\"=\"file:///[root_dir]\". \n",
    "# root_dir is optional: if you don't set it, you will have no \"root\" folder and on load/save\n",
    "# you will have to specify the full path to files or directories\n",
    "CONN_LOCAL = {\"type\": \"local\", \"url\": \"file:///d/data\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:04:36.808756Z",
     "start_time": "2019-07-10T13:04:36.805623Z"
    }
   },
   "outputs": [],
   "source": [
    "# for a distributed file system you need to specify the url in format \"schema://host[/root_dir]\" \n",
    "CONN_DFS = {\"type\": \"dfs\", \"url\": \"adl://your_adls_name.azuredatalakestore.net\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:04:44.401117Z",
     "start_time": "2019-07-10T13:04:44.397635Z"
    }
   },
   "outputs": [],
   "source": [
    "# for JDBC connections you need to specify type=\"jdbc\", url is a mandatory valid JDBC connection string.\n",
    "# the rest parameters are optional and will be passed to df.read.option(param, value) or \n",
    "# spark.read.option(param, value) \n",
    "# typically, driver, user and password are needed to handle JDBC connections from Spark\n",
    "CONN_JDBC = {\n",
    "    \"type\": \"jdbc\",\n",
    "    \"url\": \"jdbc:postgresql://yourservername:5432/yourdatabasename\", \n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": \"user\",\n",
    "    \"password\": \"******\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data - Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:11:05.925058Z",
     "start_time": "2019-07-10T13:11:05.918640Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is just an example - to show you different ways of how to load the data\n",
    "if False:\n",
    "    # local filesystem: single file\n",
    "    df = load(CONN_LOCAL, \"dir/file.csv\", a_format=\"csv\", a_row_count=True)\n",
    "    df = load(CONN_LOCAL, {\"name\": \"dir/file.csv\", \"format\": \"csv\"}, a_row_count=True)\n",
    "\n",
    "    # local filesystem: folder\n",
    "    df = load(CONN_LOCAL, \"dir/\", a_format=\"csv\",a_row_count=True)\n",
    "\n",
    "    # local filesystem: file mask\n",
    "    df = load(CONN_LOCAL, \"dir/*2019*.csv\", a_format=\"csv\", a_row_count=True)\n",
    "\n",
    "    # DFS: single file\n",
    "    df = load(CONN_DFS, \"/path/file.csv\", a_format=\"csv\", a_row_count=True)\n",
    "\n",
    "    # DFS: file mask\n",
    "    df = load(CONN_DFS, \"/path/*.csv\", a_format=\"csv\", a_row_count=True)\n",
    "\n",
    "    # DFS: folder\n",
    "    df = load(SOURCE_MAIN, \"/path/dir\", a_format=\"csv\", a_row_count=True)\n",
    "\n",
    "    # JDBC\n",
    "    df = load(SOURCE_JDBC, \"table_name\", a_row_count=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:30:20.149565Z",
     "start_time": "2019-07-10T13:30:19.823301Z"
    }
   },
   "outputs": [],
   "source": [
    "TABLE1 = {'name': './../data/dir', 'format': 'parquet'}\n",
    "\n",
    "# format attribute is mandatory.  It can be - \"csv\", \"csv.gz\", \"parquet\".  \n",
    "# For \"csv\" and \"csv.gz\" you need to specify \"header\": True/False \n",
    "# and optionally \"schema\"\n",
    "\n",
    "df1 = load(CONN_LOCAL, TABLE1, a_row_count=True, a_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:14:25.508240Z",
     "start_time": "2019-07-10T13:14:24.968768Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show(df1, a_limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:15:50.746557Z",
     "start_time": "2019-07-10T13:15:45.834261Z"
    }
   },
   "outputs": [],
   "source": [
    "TABLE_DFS = {'name': '/dest_path/dest_table', 'format': 'csv', 'header': True}\n",
    "save(df1, CONN_DFS, TABLE_DFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:17:07.285351Z",
     "start_time": "2019-07-10T13:16:55.431143Z"
    }
   },
   "outputs": [],
   "source": [
    "save(df1, CONN_JDBC, \"public.temp_table\", a_fast_write=False)  # without fast wrire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:17:54.212880Z",
     "start_time": "2019-07-10T13:17:40.835719Z"
    }
   },
   "outputs": [],
   "source": [
    "save(df1, CONN_JDBC, \"public.temp_table\")  # with fast wrire - default\n",
    "# WARNING!   THE TABLE SHOULD ALREADY EXIST IN POSTGRES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:28:45.093452Z",
     "start_time": "2019-07-10T13:28:45.089105Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ps(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:31:35.056121Z",
     "start_time": "2019-07-10T13:31:35.020572Z"
    }
   },
   "outputs": [],
   "source": [
    "# possible parameters:\n",
    "#  a_select\n",
    "#  a_drop\n",
    "#  a_replace, \n",
    "#  a_rename,\n",
    "#  a_add,\n",
    "#  a_distinct,\n",
    "#  a_filter_df, a_filter_columns, \n",
    "#  a_filter_not_df, a_filter_not_columns,\n",
    "#  a_where, \n",
    "#  a_select_end, \n",
    "#  a_drop_end,\n",
    "#  a_order_by\n",
    "\n",
    "df2 = change(df1,  \n",
    "             a_select=[\"field1\", \"field2\", \"field3\"],\n",
    "             a_add={\"DUMMY\": \"1\"},\n",
    "             a_where=\"some_key <= 1000\", \n",
    "             a_replace={\"field1\": \"cast(field1 as double)\"},  # SQL expression\n",
    "             a_rename={\"field1\": \"field2\"},\n",
    "             a_order_by={\"field1\": \"desc\"},\n",
    "             a_verbose_level=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:34:57.732077Z",
     "start_time": "2019-07-10T13:34:57.727964Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ps(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:36:42.403464Z",
     "start_time": "2019-07-10T13:36:41.959090Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show(sql(\"select * from {0} order by field1 desc\", df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:37:54.636972Z",
     "start_time": "2019-07-10T13:37:52.536692Z"
    }
   },
   "outputs": [],
   "source": [
    "cache(df1, a_row_count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:39:35.146762Z",
     "start_time": "2019-07-10T13:39:35.126414Z"
    }
   },
   "outputs": [],
   "source": [
    "uncache_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:40:23.756120Z",
     "start_time": "2019-07-10T13:40:23.753194Z"
    }
   },
   "source": [
    "## Temp Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:40:45.355370Z",
     "start_time": "2019-07-10T13:40:45.336963Z"
    }
   },
   "outputs": [],
   "source": [
    "name = temp_table(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T13:41:43.943673Z",
     "start_time": "2019-07-10T13:41:41.560267Z"
    }
   },
   "outputs": [],
   "source": [
    "show(groupby_count(df1, \"DUMMY\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
